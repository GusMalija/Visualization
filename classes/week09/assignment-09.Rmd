---
title: "Assignment 9. Data visualization"
author: "Augustine Malija"
date: "11/11/2020"
output: html_document
---

```{r, message = FALSE, warning = FALSE}
library(tidyverse) #for data visualization
library(lubridate) #for wrangling dates
library(legislatoR) #for accessing the dataset
library(dplyr) #for data manipulation
library(broom) #for presenting regression results as a table
library(viridis) #for coloring heatmaps

options(scipen = 999) #for consistent results
memory.limit(size = 9000)
```


<br>

## 1. Fetching data on members of the 115th United States Congress

The `legislatoR` package (https://github.com/saschagobel/legislatoR) provides access to the Comparative Legislators Database (CLD). The CLD includes political, sociodemographic, career, online presence, public attention, and visual information (organized in separate tables, very similar to a proper database structure) for over 45,000 contemporary and historical politicians from ten countries.

Install the package (either from CRAN or GitHub) and use it to compile the following data into one dataset:

a) The political data for the 115th session of the US House of Representatives
b) The data on daily user traffic on individual Wikipedia biographies (use it to compute the average number of daily page views per representative between January 3, 2017 and January 3, 2019 and match the variable to the dataset)
c) The information on the total number of sessions served by representative (compute it by counting the number of entries in the political table when grouped by representative).

```{r, message=FALSE}
#a.extracting political data of the US house of representatives
political <- get_political(legislature = "usa_house") %>% 
  #filtering data for the 115th session
  filter(session == 115)

#b.extracting data on daily user traffic on individual wikipedia biographies
individual_wiki <- get_traffic(legislature = "usa_house") 

#computing the average views variable separately
average_views <- individual_wiki%>% 
  #filtering the specified range of dates
  filter(date>="2017-01-03" & date<="2019-01-03") %>% 
  #grouping by representative
  group_by(pageid) %>%
  #computing the daily mean
  summarise(avg_views = mean(traffic))


#c. computing total number of sessions served by a representative
sessions_served <- get_political(legislature = "usa_house") %>% 
  #grouping by representative
  group_by(pageid) %>% 
  #counting the number of entries
  summarise(number_sessions = n())


#compiling that data into one dataset
compiled <- left_join(sessions_served, political, by = "pageid") %>% 
  #right joining with average views dataframe
  right_join(average_views, by = "pageid")
```

<br> 

## 2. Exploring the dataset

Explore the dataset using visual means, following the guidelines of good visualization. Provide three different visualizations. One visualization is entirely up to you. The two others should give any two of the following:

a) gender or ethnicity distribution by party (Democrat/Republican; ignore the others)
b) age distribution by state in which the representative's district is located (limit to states with 10+ representatives)
c) top 10 representatives according to average daily page views
d) log mean page views vs. the number of sessions served

Transform the variables if needed (e.g., categorize continuous variables, pool residual categories into one, etc.).

a. Gender/Ethnicity Distribution by Party
```{r}
#Observing that I do not have Gender and Ethnicity variables in my complied datasets, I extract them from the core dataset
gender_ethn <- get_core(legislature = "usa_house") %>% 
  #extracting gender and ethnicity variables
  select(pageid, name, sex, ethnicity) %>% 
  #joining it with the compiled dataset
  left_join(compiled, by = "pageid")

#plotting with a density plot
#subsetting on democrats and republicans
ggplot(subset(gender_ethn, party %in% c("R", "D")),
              aes(x = number_sessions,
                  #specifying ethnicity to distinct plots through color and shape
                  color = ethnicity,
                  linetype = ethnicity))+
  #picking density plot as a geom
         geom_line(stat = "density") +
         labs(title = "Ethnicity Distribution by Party",
              subtitle = "Democrats and Republicans",
              x = "Number of Sessions Served",
              y = "Density") +
  #making the background of the plot lighter
         theme_light()+
  #specifying colors
  scale_color_brewer(palette = "Set2",
                     name = "Ethnicity")+
  scale_linetype_discrete(name = "Ethnicity")
```

<span style="color:blue">
Black and Asian legislators have big spikes in the number of views closer to five. In general, the plot is right skewed suggesting for a log transformation of the variable.
</span>

c. Top ten Representatives according to average daily page views
```{r}
#sorting the top ten representatives
top_ten <- gender_ethn %>% 
  #selecting variables of my interest
  select(name, avg_views) %>% 
  #sorting in a descending order
  arrange(desc(avg_views)) %>% 
  #picking only the top ten
  slice(1:10) %>% 
  #printing a table
  print.data.frame(.)
```


### Plotting top ten candidates through faceting
```{r}
ggplot(top_ten, 
       aes(x = name, y = avg_views, fill = name)) +
  geom_col(position = position_dodge())

```


<span style="color:blue">
I observe that the aformentioned four candidates stand in the top list of average views.
</span>


### Using heatmaps to plot the correlation between number of sessions and average page views
```{r, message=FALSE}

cors_sessions_views <- individual_wiki %>% 
  #selecting date column
  select(pageid, date) %>% 
  #joining datasets
  right_join(gender_ethn) %>%
  #grouping by ethnicity and date
  group_by(ethnicity, date) %>% 
  #calculating correlations 
  summarise(cor = cor(number_sessions, avg_views)) %>% 
  #filtering out ethnicity values that have NAs
  filter(!is.na(ethnicity)) %>%
  #arranging in a descending order
  arrange(desc(cor))

#plotting a heatmap
ggplot(cors_sessions_views,
       aes(x = date, y = ethnicity, fill = cor)) +
  geom_tile() +
  #using color gradient schemes to better distinguish values in the plot
  scale_fill_viridis(option = "inferno", name = "Correlation\ncoefficient",
                     #extending correlation limits respectively
                     limits = c(-0.2,0.3)) +
  labs(x ="",
       y ="",
       title = "Correlation Between Number of Sessions Served and Daily Average Views") +
  #making the plot's background lighter
  theme_light() +
  theme(panel.grid = element_blank(),
        #positioning the legend to the bottom
        legend.position = "bottom",
        #adjusting the width of the legend to three centimetres
        legend.key.width = unit(3, "cm"),
        #omitting the border line around the plot
        panel.border = element_blank(),
        #omitting axis ticks infront of names
        axis.ticks = element_blank()) +
  #reducing space between plot and labels
  coord_cartesian(expand = 0)
```

<span style="color:blue">
This heatmap shows the stregth of the correlation between the number of sessions served and the daily average page views as they trend through time.
</span>


<br> 

## 3. Modeling page views

Finally, model the log number of mean page views as a function of the following variables: 

  - number of sessions served, 
  - party membership (Democrat/Republican/Independent)
  - key political position (a dummy which takes the value 1 if the representative is one of the following: speaker, majority/minority leader/whip)
  - age
  - gender
  - ethnicity (white/non-white)

A linear model is just fine. Present the results of your model in both a table and a coefficient plot!

```{r, warning=FALSE}
#extracting age variable
#setting todays date to replace with values containing NAs
today <- ymd("2020-11-16")

#obtaining core data with birth and death variables
age_data <- get_core(legislature = "usa_house") %>%
  #replacing NA values with today's date
  #operating under the assumption that these legislators are still alive
  mutate(death = replace_na(death, today)) %>% 
  #calculating age with 366 days since 2020 is a leap year
  mutate(age = interval(birth, death)/days(366)) %>%
  #I observe there are six observations above 120 years old
  #this is because death dates are missing. And these are for people born before the past four generations
  #I assume that the oldest person lives 120 years. I then replace these variables with 120 years
  mutate(age = replace(age, age>120, 120)) %>% 
  #selecting needed variables
  select(pageid, age, sex, ethnicity)

#specifying party membership and political positions
#for consistency, I pick observations for 155th session
membership_poltpos <- political %>% 
  filter(party %in% c("R","D","I"))

#writing a function to add a dummy variable of specified variables for political position
#looping through to specify from frist row to last row
for (row in 1:nrow(membership_poltpos)){
  #renaming variables
  speaker <- membership_poltpos[row,"house_speaker"]
  maj_leader <- membership_poltpos[row, "house_majority_leader"]
  min_leader <- membership_poltpos[row, "house_minority_leader"]
  maj_whip <- membership_poltpos[row, "house_majority_whip"]
  min_whip <- membership_poltpos[row, "house_minority_whip"]
  #applying the or argument between variables to return dummy variables  
  if (speaker == T | maj_leader == T | min_leader == T | maj_whip == T | min_whip == T){membership_poltpos$pol_position[row] <- 1}
  else{membership_poltpos$pol_position[row]<- 0}
}

#picking only wanted variables
membership_poltpos <- membership_poltpos %>% 
  select(pageid, pol_position, party)

#log transforming and compiling datasets
log_page_views <- compiled %>%
  #adding a log of mean page views variable
  mutate(log_views = log(avg_views)) %>% 
  select(pageid, number_sessions, log_views) %>% 
  #joining the datasets
  left_join(age_data, by = "pageid") %>% 
  left_join(membership_poltpos, by = "pageid") %>% 
  #replacing infinity values in log_views variables to NAs for swift regression
  mutate(log_views = replace(log_views, !is.finite(log_views), NA))
  
#running the linear regression
log_linear <- lm(log_views ~ number_sessions + party + pol_position + age + sex + ethnicity, data = log_page_views)

#presenting results in a data frame
table <- tidy(log_linear)
#presenting them as a table
table
```


### Doing a Coefficient Plot out of the table
```{r}
table %>% 
  #filtering out the intercept
  filter(term != "(Intercept)") %>% 
  ggplot(aes(x = term, y = estimate))+
  #adding a line for the null effect after dropping the intercept
  geom_hline(yintercept = 0, color = "purple")+
  #indicating the points for our values
  geom_point()+
  #adding confidence intervals
  geom_linerange(aes(ymin = estimate - 1.96*std.error, ymax = estimate + 1.96*std.error))+
  geom_linerange(aes(ymin = estimate - 1.65*std.error, ymax = estimate + 1.65*std.error), size = 1)+
  #naming my axes
  labs(x ="Variables",
       y = "OLS Coefficient with 90% and 95% Confidence Intervals")+
  #flipping the coordinates
  coord_flip()+
  #making the background lighter and ready for presentation
  theme_light()
```

<span style="color:blue">
I learn that of all statistically significant variables, political position is a strikingly significant variable determining page views. Conversely, ethnicity, and party are insignificant factors.
</span>
